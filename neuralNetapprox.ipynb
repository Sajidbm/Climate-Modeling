{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd6361f",
   "metadata": {},
   "source": [
    "Our task now is to use a neural network $f_{\\theta}$ to approximate the simulator $\\mathcal{P}$ created with the Runge-Kutta (RK4) method. The properties and functionality of the simulator can be found in the `lorenz63.py` file. We will use a Multilayer Perceptron (MLP) neural network. The main task is to learn the parameter(s) $\\theta$ that approxiates $\\mathcal{P}$: $$f_{\\theta} \\approx  \\mathcal{P}$$ The idea is to try to emulate/approximate the Runge-Kutta scheme using a neural network. In RK4, we used previous states to explicitly compute the next state. Now, blind to the scheme, we will try to learn the parameters $\\theta$ of our neural network from the trajectory by having the NN predict the next state given the current state and learned parameters. I will try both cases: with and without noise in the Lorenz simulator. \n",
    "\n",
    "For practitioners, what I am trying to do is learning the discrete flow map using an MLP based emulator. Lorenz '63 is a first order autonomous ODE (no mention of time in the equations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9bfe7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "# deep learning framework, we'll use their MLP architechtrue \n",
    "import equinox as eqx\n",
    "\n",
    "# gradient processing library, for adam optimizer \n",
    "import optax\n",
    "\n",
    "# training monitoring \n",
    "from tqdm import tqdm\n",
    "\n",
    "# generate data\n",
    "from Study.lorenz63JAX import rollout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_0_set = jax.random.normal(jax.random.PRNGKey(0), (9, 3))\n",
    "lorenzStepper = LorenzSimulatorK4()\n",
    "iterations = 7000\n",
    "\n",
    "rollout_func = rollout(lorenzStepper, iterations, include_init=False)\n",
    "dataset = jax.vmap(rollout_func)(u_0_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d18a0d",
   "metadata": {},
   "source": [
    "There needs to be a more robust way to check this but the initial phase of the trajectory is quite turbulent according to my experience, susceptible to initial conditions. So we will let go off the first 2000 datapoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d9138",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = jax.vmap(rollout_func)(u_0_set)[:, 2000:]\n",
    "\n",
    "# divide up into training and testing (we use the first 6 trajectories for training and leave the remaining for testing)\n",
    "training_set, testing_set = dataset[:6], dataset[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0da0fd7",
   "metadata": {},
   "source": [
    "$$\\renewcommand{\\cP}{\\mathcal{P}}$$\n",
    "\n",
    "### Online learning\n",
    "\n",
    "Recall how the simulator/stepper works $\\cP(u_n) = u_{n+1}$. Our emulator is also a stepper function. $$f_{\\theta}(u_n) = \\hat{u_n}$$ Because of our dataset above, we already have $$\\text{Dataset}: \\{ u_0, \\cP(u_0),\\cdots, \\cP(u_{n-1})\\}$$ Our MLP based emulator will first make a prediction on $u_0$ and get $f_\\theta(u_0) = \\hat{u_1}$. Next, it will compute the error $$ e = ||\\hat{u_1} -   \\cP(u_0)||^2/n = ||\\hat{u_1} -  u_1||^2/n$$ Once we have the error, we can gradient descent $\\theta$ to minimize the error. Once that is done we have a new theta $$\\theta \\text{ gradient descent} \\to \\hat{\\theta}$$ Now that we have a theta (which is hopefully better) we can do prediction on $u_1$ and get $f_\\theta(u_1) = \\hat{u_2}$ and do the whole thing again. Now note that this is technically a time series so we have a create a moving window that takes an input $u_n$ to $f_{theta}$ given the current best parameter $\\theta$ and check the prediction $f_{theta}(u_n) = \\hat{u_{n+1}}$ against $u_{n+1}$. So each window must have two data points from our training set $(u_n, u_{n+1})$ \n",
    "\n",
    "A few things I will need to keep in mind:\n",
    "\n",
    "- Normalization: scaling with mean 0, variance 1\n",
    "- Batching: instead of doing the process like data is arriving one at a time, for the sake of time, we can batch the whole thing and run one gradient step. \n",
    "- A residual network that learns the change: $$u_{n+1} = u_n + \\Delta t \\cdot f_\\theta(u_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb2811",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
